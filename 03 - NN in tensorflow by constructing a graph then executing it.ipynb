{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a code cell that contains Python code\n",
    "# we usually start with the imports\n",
    "# these are the imports we usually use for machine learning\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 20\n",
    "num_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data, like last time\n",
    "\n",
    "corpus, targets = zip(*[(movie_reviews.raw(fileid), category)\n",
    "                         for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)])\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=num_features)\n",
    "bows = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert targets to numbers\n",
    "targets = np.array([0 if target == 'neg' else 1 for target in targets])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bows, targets, test_size=0.1, shuffle=True)\n",
    "\n",
    "# the problem: we have sparse arrays, but neural network need dense arrays!\n",
    "# the solution will be word embeddings, here we just convert to dense arrays\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros:0' shape=(10, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are building a computational graph, tensors are not evaluated beforehand\n",
    "v = tf.zeros((10, 10))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(10, 10) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apple/anaconda3/envs/summer_school/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'W1:0' shape=(20, 5000) dtype=float32_ref>,\n",
       " <tf.Variable 'b1:0' shape=(20, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'W2:0' shape=(1, 20) dtype=float32_ref>,\n",
       " <tf.Variable 'b2:0' shape=(1, 1) dtype=float32_ref>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables are for the parameters (weights) for the NN\n",
    "W1 = tf.Variable(tf.random_normal((num_neurons, num_features), stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal((num_neurons, 1), stddev=0.01), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal((1, num_neurons), stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal((1, 1), stddev=0.01), name='b2')\n",
    "W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(5000, ?) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# placeholders these are for the input data\n",
    "X = tf.placeholder(tf.float32, shape=(num_features, None))\n",
    "y = tf.placeholder(tf.float32, shape=(1, None))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network itself\n",
    "z = tf.add(tf.matmul(W1, X), b1)\n",
    "print(z)\n",
    "a = tf.nn.relu(z)\n",
    "print(a)\n",
    "z = tf.add(tf.matmul(W2, a), b2)\n",
    "print(z)\n",
    "#a = tf.nn.sigmoid(z) # this is already in the loss\n",
    "a = z\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need an optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(X, y, minibatch_size):\n",
    "\n",
    "# shuffling\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "    \n",
    "    mini_batches = [(X[i*minibatch_size:(i+1)*minibatch_size], y[i*minibatch_size:(i+1)*minibatch_size])\n",
    "                   for i in range(X.shape[0] // minibatch_size)]\n",
    "# we lost some examples at the end, doesn't really matter for this example\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 32\n",
    "# we need a Session to run the graph\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"Loss at start: {}\".format(sess.run(loss, feed_dict={X: X_train.T, y: y_train[None, :]})))\n",
    "    for epoch in range(num_epochs):\n",
    "        # minibatch training\n",
    "        epoch_loss = 0\n",
    "        num_minibatches = X_train.shape[0] // minibatch_size\n",
    "        for X_mini, y_mini in minibatches(X_train, y_train, minibatch_size):\n",
    "            # important: we have to transpose here! each example is in a column\n",
    "            _, mini_loss = sess.run([optimizer, loss], feed_dict={X: X_mini.T, y: y_mini[None, :]})\n",
    "            epoch_loss += mini_loss\n",
    "        epoch_loss /= num_minibatches\n",
    "        predictions = sess.run(a, feed_dict={X: X_train.T, y: y_train[None, :]})\n",
    "        predictions = np.array([0 if pred < 0 else 1 for pred in predictions.ravel()])\n",
    "        accuracy = (y_train == predictions).sum() / len(y_train)\n",
    "        print(\"Loss in epoch {}: {}, accuracy: {}\".format(epoch, epoch_loss, accuracy))\n",
    "    # accuracy on test set\n",
    "    predictions = sess.run(a, feed_dict={X: X_test.T, y: y_test[None, :]})\n",
    "    predictions = np.array([0 if pred < 0 else 1 for pred in predictions.ravel()])\n",
    "    accuracy = (y_test == predictions).sum() / len(y_test)\n",
    "    print(\"Accuracy on test set: {}\".format(accuracy))\n",
    "    tf.summary.FileWriter('nn_graph', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
