{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a code cell that contains Python code\n",
    "# we usually start with the imports\n",
    "# these are the imports we usually use for machine learning\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 20\n",
    "num_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data, like last time\n",
    "\n",
    "corpus, targets = zip(*[(movie_reviews.raw(fileid), category)\n",
    "                         for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)])\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=num_features)\n",
    "bows = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert targets to numbers\n",
    "targets = np.array([0 if target == 'neg' else 1 for target in targets])\n",
    "\n",
    "bows = bows.astype(np.float32)\n",
    "targets = targets.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(bows, targets, test_size=0.1, shuffle=True)\n",
    "\n",
    "# the problem: we have sparse arrays, but neural network need dense arrays!\n",
    "# the solution will be word embeddings, here we just convert to dense arrays\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can see the tensor instantly\n",
    "tf.ones((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6, shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also see the results of an operation instantly\n",
    "tf.add(tf.constant([1, 2, 3]), tf.constant([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((4, 4)) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'W1:0' shape=(20, 5000) dtype=float32, numpy=\n",
       " array([[ 0.00861787,  0.02128106,  0.00960503, ...,  0.01153171,\n",
       "         -0.01031314, -0.00281827],\n",
       "        [ 0.00824988,  0.00486534, -0.00140077, ..., -0.0033846 ,\n",
       "          0.00329555,  0.0065271 ],\n",
       "        [ 0.02080767, -0.00551045,  0.00208974, ...,  0.00487724,\n",
       "         -0.00768933, -0.01369358],\n",
       "        ...,\n",
       "        [-0.03269031, -0.00355082,  0.00315598, ...,  0.01347431,\n",
       "         -0.00355484,  0.01264856],\n",
       "        [-0.01359317, -0.00880538,  0.00645966, ..., -0.00399274,\n",
       "         -0.00305909,  0.00597724],\n",
       "        [ 0.00789742,  0.008413  , -0.01262239, ...,  0.01687056,\n",
       "          0.00802961,  0.00191226]], dtype=float32)>,\n",
       " <tf.Variable 'b1:0' shape=(20, 1) dtype=float32, numpy=\n",
       " array([[-0.02093641],\n",
       "        [-0.0023646 ],\n",
       "        [-0.00238637],\n",
       "        [-0.00131463],\n",
       "        [ 0.00071911],\n",
       "        [-0.00318027],\n",
       "        [ 0.0073009 ],\n",
       "        [ 0.00193804],\n",
       "        [ 0.0144193 ],\n",
       "        [ 0.00543289],\n",
       "        [ 0.00886858],\n",
       "        [ 0.00181041],\n",
       "        [ 0.00204222],\n",
       "        [ 0.02299558],\n",
       "        [ 0.00121133],\n",
       "        [-0.01289714],\n",
       "        [-0.00425186],\n",
       "        [ 0.00549004],\n",
       "        [-0.00776517],\n",
       "        [-0.01809266]], dtype=float32)>,\n",
       " <tf.Variable 'W2:0' shape=(1, 20) dtype=float32, numpy=\n",
       " array([[-0.00096033,  0.02013952, -0.00072787,  0.0124417 , -0.01918222,\n",
       "          0.0199432 ,  0.00870001,  0.00185575, -0.01518136, -0.00525579,\n",
       "         -0.01260701,  0.02139573, -0.00796839, -0.0144563 ,  0.00120091,\n",
       "         -0.00476017,  0.00126254,  0.00858311,  0.00336941, -0.00637602]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'b2:0' shape=(1, 1) dtype=float32, numpy=array([[0.00171679]], dtype=float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables are for the parameters (weights) for the NN\n",
    "W1 = tf.Variable(tf.random_normal((num_neurons, num_features), stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal((num_neurons, 1), stddev=0.01), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal((1, num_neurons), stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal((1, 1), stddev=0.01), name='b2')\n",
    "W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network itself\n",
    "def forward_pass(X):\n",
    "    z = tf.add(tf.matmul(W1, X), b1)\n",
    "    a = tf.nn.relu(z)\n",
    "    z = tf.add(tf.matmul(W2, a), b2)\n",
    "    #a = tf.nn.sigmoid(z) # already in loss function\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "def loss(a, y):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(X, y, minibatch_size):\n",
    "\n",
    "# shuffling    \n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "    \n",
    "    mini_batches = [(X[i*minibatch_size:(i+1)*minibatch_size], y[i*minibatch_size:(i+1)*minibatch_size])\n",
    "                   for i in range(X.shape[0] // minibatch_size)]\n",
    "# we lost some examples at the end, doesn't really matter for this example\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_1 := W_1 - lr*\\frac{loss}{dW_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.6551772952079773, accuracy: 0.8216666666666667\n",
      "Loss in epoch 1: 0.4975118339061737, accuracy: 0.6933333333333334\n",
      "Loss in epoch 2: 0.40052077174186707, accuracy: 0.9477777777777778\n",
      "Loss in epoch 3: 0.2790760397911072, accuracy: 0.8588888888888889\n",
      "Loss in epoch 4: 0.26597705483436584, accuracy: 0.9283333333333333\n",
      "Loss in epoch 5: 0.20803622901439667, accuracy: 0.9672222222222222\n",
      "Loss in epoch 6: 0.2595866322517395, accuracy: 0.7794444444444445\n",
      "Loss in epoch 7: 0.15296688675880432, accuracy: 0.9755555555555555\n",
      "Loss in epoch 8: 0.035927943885326385, accuracy: 0.9972222222222222\n",
      "Loss in epoch 9: 0.009744838811457157, accuracy: 0.9988888888888889\n",
      "Accuracy on test set: 0.875\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "minibatch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "# minibatch training\n",
    "    epoch_loss = 0\n",
    "    num_minibatches = X_train.shape[0] // minibatch_size\n",
    "    for X_mini, y_mini in minibatches(X_train, y_train, minibatch_size):\n",
    "# we use a GradientTape to record the gradient for each minibatch\n",
    "        with tf.GradientTape() as t:\n",
    "            # important: we have to transpose here! each example is in a column\n",
    "            mini_loss = loss(forward_pass(X_mini.T), y_mini[None, :])\n",
    "        # update the weights\n",
    "        dW1, db1, dW2, db2 = t.gradient(mini_loss, [W1, b1, W2, b2])\n",
    "        W1.assign_sub(learning_rate * dW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * dW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        epoch_loss += mini_loss\n",
    "    epoch_loss /= num_minibatches\n",
    "    predictions = forward_pass(X_train.T)\n",
    "    predictions = np.array([0 if pred < 0 else 1 for pred in tf.squeeze(predictions)])\n",
    "    accuracy = (y_train == predictions).sum() / len(y_train)\n",
    "    print(\"Loss in epoch {}: {}, accuracy: {}\".format(epoch, epoch_loss, accuracy))\n",
    "# accuracy on test set\n",
    "predictions = forward_pass(X_test.T)\n",
    "predictions = np.array([0 if pred < 0 else 1 for pred in tf.squeeze(predictions)])\n",
    "accuracy = (y_test == predictions).sum() / len(y_test)\n",
    "print(\"Accuracy on test set: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
